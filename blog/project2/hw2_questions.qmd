---
title: "Poisson Regression Examples"
author: "Timon Ortwein"
date: today
callout-appearance: minimal
---

## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.

### Data Analysis

### Data

Let's begin by examining the distribution of patents between Blueprinty customers and non-customers. This will give us our first insight into whether there might be a relationship between using Blueprinty's software and patent success.

```{python}
#| code-fold: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.special import factorial, gammaln
from scipy import optimize
import statsmodels.api as sm

blueprinty_df = pd.read_csv('blueprinty.csv')
print(blueprinty_df.head())
```

#### Patent Distribution Analysis

```{python}
#| code-fold: true
# Plot patent distributions
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.hist(blueprinty_df[blueprinty_df['iscustomer'] == 0]['patents'], bins=20, color='red')
plt.title('Patents Distribution - Non-Customers')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
plt.hist(blueprinty_df[blueprinty_df['iscustomer'] == 1]['patents'], bins=20, color='blue')
plt.title('Patents Distribution - Customers')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.show()

# Calculate mean patents
mean_patents = blueprinty_df.groupby('iscustomer')['patents'].mean()
print("Mean Patents by Customer Status:")
print(f"Non-Customers (0): {mean_patents[0]:.2f}")
print(f"Customers (1): {mean_patents[1]:.2f}")
```

The histograms reveal right-skewed distributions for both groups, with customers showing higher mean patent counts. This suggests a potential association between software usage and patent success, though confounding factors must be considered.

#### Regional and Age Distribution

Next, we examine whether customer status is related to region or firm age, which could confound our analysis.

```{python}
#| code-fold: true
# Analyze regional distribution
plt.figure(figsize=(10, 5))
regional_dist = pd.crosstab(blueprinty_df['region'], blueprinty_df['iscustomer'], normalize='columns') * 100
regional_dist.plot(kind='bar')
plt.title('Regional Distribution by Customer Status')
plt.xlabel('Region')
plt.ylabel('Percentage')
plt.legend(['Non-Customers', 'Customers'])
plt.show()

print("\nRegional Distribution (%):")
print(regional_dist)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.boxplot(data=blueprinty_df[blueprinty_df['iscustomer'] == 0], y='age')
plt.title('Age Distribution - Non-Customers')
plt.ylabel('Age (years)')

plt.subplot(1, 2, 2)
sns.boxplot(data=blueprinty_df[blueprinty_df['iscustomer'] == 1], y='age')
plt.title('Age Distribution - Customers')
plt.ylabel('Age (years)')
plt.show()

print("Mean Age by Customer Status:")
print(blueprinty_df.groupby('iscustomer')['age'].mean())
```

Looking at the regional distribution, we can observe notable variations in customer adoption across different regions. The northeast region shows higher proportions of Blueprinty customers than others, suggesting potential geographic clustering. This could be due to various factors such as local marketing efforts or other.
The age comparison between customers and non-customers reveals that there is not much variation in age. The boxplots show that Blueprinty's customer base does not significantly tend to differ in age composition from non-customers. 
The differences in regional distribution underscores the importance of controlling for this variables in our subsequent analysis. Without accounting for these factors, we might incorrectly attribute differences in patent success rates to Blueprinty's software when they could be partially explained by this underlying characteristic.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

For a sample of n independent observations $Y_1, ..., Y_n$ from a Poisson distribution with parameter $\lambda$, the likelihood function is:

$L(\lambda|Y) = \prod_{i=1}^n f(Y_i|\lambda) = \prod_{i=1}^n \frac{e^{-\lambda}\lambda^{Y_i}}{Y_i!}$

This represents the probability of observing our data Y given the parameter $\lambda$. The product comes from the independence of observations.


```{python}
#| code-fold: true
def poisson_loglikelihood(lambda_, Y):
    Y = np.array(Y)
    n = len(Y)
    sum_Y = np.sum(Y)
    sum_log_factorial = np.sum(np.log(factorial(Y)))
    return -n * lambda_ + sum_Y * np.log(lambda_) - sum_log_factorial

# Calculate MLE
Y = blueprinty_df['patents'].values
lambda_values = np.linspace(0.1, 10, 100)

log_likelihoods = [poisson_loglikelihood(lambda_, Y) for lambda_ in lambda_values]

plt.figure(figsize=(10, 6))
plt.plot(lambda_values, log_likelihoods)
plt.xlabel('λ (lambda)')
plt.ylabel('Log-likelihood')
plt.title('Log-likelihood Function for Poisson Model')
plt.grid(True)
plt.show()

sample_mean = round(np.mean(Y),2)
print("MLE for lambda:" sample_mean)
```

The plot above shows the log-likelihood as a function of $\lambda$. The maximum occurs at the sample mean, which is the MLE for the Poisson rate parameter.

We can also confirm this by direct optimization:

```{python}
#| code-fold: true
def neg_poisson_loglikelihood(lambda_, Y):
    return -poisson_loglikelihood(lambda_, Y)

result = optimize.minimize(neg_poisson_loglikelihood, x0=np.mean(Y), args=(Y,), method='BFGS')
print(f"MLE of λ from optimization: {result.x[0]:.2f}")
```

### Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that 
$Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The 
interpretation is that the success rate of patent awards is not constant across all 
firms ($\lambda$) but rather is a function of firm characteristics $X_i$. 
Specifically, we will use the covariates age, age squared, region, and whether the 
firm is a customer of Blueprinty.

To account for firm characteristics, we fit a Poisson regression model where the expected number of patents depends on age, age squared, region, and customer status.

```{python}
#| code-fold: true
def neg_log_likelihood(beta, Y, X):
    beta = np.asarray(beta, dtype=np.float64)
    Y = np.asarray(Y, dtype=np.float64)
    X = np.asarray(X, dtype=np.float64)

    eta = X @ beta
    eta = np.clip(eta, -20, 20)
    lambda_ = np.exp(eta)

    log_likelihood = np.sum(Y * eta - lambda_ - gammaln(Y + 1))
    return -log_likelihood
```


Let's estimate the Poisson regression model and analyze the results:

```{python}
#| code-fold: true
from scipy.optimize import minimize

Y = blueprinty_df['patents'].astype(float).values
X = blueprinty_df[['age', 'region', 'iscustomer']].copy()

X['age'] = (X['age'] - X['age'].mean()) / X['age'].std()
X['age_squared'] = X['age'] ** 2

region_dummies = pd.get_dummies(X['region'], prefix='region', drop_first=True)

X_final = pd.concat([
    pd.Series(1.0, index=X.index, name='intercept'),
    X[['age', 'age_squared', 'iscustomer']],
    region_dummies], axis=1)

X_np = X_final.to_numpy(dtype=np.float64)

initial_beta = np.zeros(X_np.shape[1])
result = minimize(
    neg_log_likelihood,
    x0=initial_beta,
    args=(Y, X_np),
    method='L-BFGS-B'
)

beta_hat = result.x
hessian_inv = result.hess_inv
cov_matrix = hessian_inv.todense()
standard_errors = np.sqrt(np.diag(cov_matrix))

summary_df = pd.DataFrame({
    'Coefficient': beta_hat,
    'Std. Error': standard_errors
}, index=X_final.columns)

summary_df = summary_df.round(4)
print("\nPoisson Regression Results:")
print(summary_df)
```


### Comparison with Built-in GLM

Next we double check our result with the built in GLM function:

```{python}
#| code-fold: true
import statsmodels.api as sm

df = blueprinty_df.copy()
df['age'] = (df['age'] - df['age'].mean()) / df['age'].std()
df['age_squared'] = df['age'] ** 2

region_dummies = pd.get_dummies(df['region'], prefix='region', drop_first=True)

model_df = pd.concat([
    df[['patents', 'age', 'age_squared', 'iscustomer']],
    region_dummies], axis=1)

y = model_df['patents'].astype(float)
X = sm.add_constant(model_df.drop(columns='patents')).astype(float)

poisson_model = sm.GLM(y, X, family=sm.families.Poisson())
poisson_results = poisson_model.fit()

print(poisson_results.summary())
```

The results from the manual calculation matches those from the built-in GLM function. This agreement provides confidence in the validity of the findings and the reliability of the modeling approach.

### Interpretation of Results

The Poisson regression results provide meaningful insights into the factors associated with patenting success among firms. Most notably, firms that use Blueprinty’s software are predicted to file more patents than comparable firms that do not use the software. This relationship remains positive even after controlling for other characteristics such as firm age and regional location, suggesting that Blueprinty’s tools may be effectively supporting innovation efforts. 
The model also reveals a non-linear relationship between age and patent output: patenting tends to increase as firms age, but eventually declines, consistent with the idea that younger firms may be more dynamic and innovative, while older firms could become less agile over time. 
I made this assumption as both age and age squared are negative and statistically significant. This pattern suggests an inverted-U relationship. The patenting activity tends to rise as firms age initially, but eventually declines, indicating that older firms may become less innovative over time. 
Finally, the analysis finds no strong or consistent evidence that region plays a significant role in driving patent activity. Once differences in firm-level factors are accounted for, geographic location does not appear to be a major determinant of patenting success.

### Average Treatment Effect Analysis

```{python}
#| code-fold: true

X_0 = X.copy()
X_1 = X.copy()
X_0['iscustomer'] = 0
X_1['iscustomer'] = 1

y_pred_0 = poisson_results.predict(X_0)
y_pred_1 = poisson_results.predict(X_1)

avg_effect = (y_pred_1 - y_pred_0).mean()
print(round(avg_effect,3))
```

To assess the practical impact of Blueprinty's software, we estimated the average treatment effect by comparing predicted patent counts for all firms as if they were customers versus non-customers.
The analysis suggests that being a customer of Blueprinty's software has a positive effect on patent success. Using a Poisson regression model, we estimated that firms using Blueprinty's software are expected to file approximately 0.793 more patents on average compared to similar firms that do not use the software, holding all other firm characteristics constant.
These results support the marketing claim that Blueprinty's software is linked to greater patenting success. This provides strong evidence that customers of Blueprinty tend to outperform non-customers in terms of patent output, even after accounting for other important firm characteristics.


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped 40,000 Airbnb listings from New York City. The goal of this analysis is to understand what factors influence the number of reviews a listing receives, using Poisson regression, which is well-suited for modeling count data.


The dataset includes variables such as price, room type, instant bookability, and review scores for cleanliness, location, and value. We begin by loading and inspecting the data to ensure it is suitable for analysis.

```{python}
df = pd.read_csv("airbnb.csv")
print(df.head())
```

The first few rows of the dataset give us a sense of the variables available and their formats. We see that the data includes both numeric and categorical variables, which will need to be processed appropriately for modeling.

### Data Preparation

Before fitting our model, we clean the data by removing rows with missing values in key columns, converting categorical variables to numeric, and ensuring all predictors are numeric. This step is crucial for the validity of our regression analysis.

```{python}
df_clean = df.dropna(subset=["number_of_reviews"])
selected_columns = [
    "number_of_reviews",
    "price",
    "room_type",
    "instant_bookable",
    "review_scores_cleanliness",
    "review_scores_location",
    "review_scores_value"
]
df_model = df_clean[selected_columns].copy()
df_model.dropna(inplace=True)

df_model["instant_bookable"] = df_model["instant_bookable"].map({"t": 1, "f": 0})

df_model = pd.get_dummies(df_model, columns=["room_type"], drop_first=True)
print(df_model.head())

```

After cleaning, our modeling dataset contains only numeric columns, with categorical variables such as room type represented as dummy variables. This ensures compatibility with the Poisson regression model.

### Maximum Likelihood Estimation by Hand

We first fit a Poisson regression model using maximum likelihood estimation manually. This approach allows us to understand the mechanics of model fitting and provides a benchmark for comparison with built-in functions. We reused the neg_log_likelihood function from the previous case.

```{python}
X = df_model.drop(columns=["number_of_reviews"]).copy()
X.insert(0, "intercept", 1)
X = X.values

y = df_model["number_of_reviews"].values

initial_beta = np.zeros(X.shape[1])

# Run optimizer using your defined neg_log_likelihood(beta, Y, X)
result = minimize(
    fun=neg_log_likelihood,
    x0=initial_beta,
    args=(y, X),
    method='L-BFGS-B'
)

# Extract results
estimated_beta = result.x
converged = result.success
message = result.message

print("Estimated coefficients:", estimated_beta)
print("Converged:", converged)
print("Message:", message)
```

The output above shows the estimated coefficients for each predictor, as well as information about the optimizer's convergence. These coefficients represent the log effect of each variable on the expected number of reviews, holding other variables constant.

### Model Fitting with Built-in GLM

To validate our custom MLE results, we also fit a Poisson regression model using the built in GLM function.

```{python}
import statsmodels.api as sm

# Prepare data for statsmodels
X_sm = df_model.drop(columns=["number_of_reviews"]).astype(float)
X_sm = sm.add_constant(X_sm)
y_sm = df_model["number_of_reviews"].astype(float)

poisson_model = sm.GLM(y_sm, X_sm, family=sm.families.Poisson())
poisson_results = poisson_model.fit()

print(poisson_results.summary())
```

The regression summary above provides the same values as our manual approach improving the confidence that the results are right.

### Interpretation of Results

Listings with instant booking enabled tend to receive more reviews, suggesting that ease of booking increases guest engagement. Cleanliness scores are also positively associated with review volume, highlighting the importance of hygiene.
Surprisingly, higher scores for location and value are linked to fewer reviews. This may reflect overlapping review dimensions or unobserved guest behavior.
Entire homes and apartments receive more reviews than private or shared rooms, likely due to greater guest preference for privacy. Price has no meaningful impact on review count.
Overall, instant booking, cleanliness, and room type are stronger predictors of review volume than pricing or location/value ratings.

## Conclusion

This analysis demonstrates the application of Poisson regression in two distinct business contexts: patent analysis for Blueprinty and review analysis for AirBnB. In both cases, we successfully modeled count data using Poisson regression.

For Blueprinty, our analysis revealed that firms using their software tend to have higher patent success rates, even after controlling for firm age and regional differences. The Poisson regression model provided valuable insights into the relationship between software usage and patent outcomes, supporting the marketing team's claims while accounting for important confounding variables.
In the AirBnB case study, we identified key factors influencing the number of reviews, with instant bookability and room type showing particularly strong effects. The analysis provides actionable insights for hosts looking to increase their listing visibility and engagement.


