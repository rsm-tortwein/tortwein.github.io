{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Multinomial Logit Model\"\n",
        "author: \"Timon Ortwein\"\n",
        "date: today\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n",
        "\n",
        "\n",
        "## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n",
        "\n",
        "Suppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n",
        "\n",
        "We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n",
        "\n",
        "$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n",
        "\n",
        "where $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n",
        "\n",
        "The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n",
        "\n",
        "$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n",
        "\n",
        "For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n",
        "\n",
        "$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n",
        "\n",
        "A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n",
        "\n",
        "$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n",
        "\n",
        "Notice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n",
        "\n",
        "$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n",
        "\n",
        "The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n",
        "\n",
        "$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n",
        "\n",
        "And the joint log-likelihood function is:\n",
        "\n",
        "$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n",
        "\n",
        "\n",
        "## 2. Simulate Conjoint Data\n",
        "\n",
        "We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n",
        "\n",
        "Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n",
        "\n",
        "The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n",
        "\n",
        "$$\n",
        "u_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n",
        "$$\n",
        "\n",
        "where the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n",
        "\n",
        "The following code provides the simulation of the conjoint data.\n",
        "\n",
        ":::: {.callout-note collapse=\"true\"}"
      ],
      "id": "240f26fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# set seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# define attributes\n",
        "brand = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\n",
        "ad = [\"Yes\", \"No\"]\n",
        "price = list(range(8, 33, 4))\n",
        "\n",
        "# generate all possible profiles\n",
        "profiles = pd.DataFrame([\n",
        "    (b, a, p) for b in brand for a in ad for p in price\n",
        "], columns=[\"brand\", \"ad\", \"price\"])\n",
        "m = len(profiles)\n",
        "\n",
        "# assign part-worth utilities (true parameters)\n",
        "b_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0}\n",
        "a_util = {\"Yes\": -0.8, \"No\": 0.0}\n",
        "def p_util(p): return -0.1 * p\n",
        "\n",
        "# number of respondents, choice tasks, and alternatives per task\n",
        "n_peeps = 100\n",
        "n_tasks = 10\n",
        "n_alts = 3\n",
        "\n",
        "# function to simulate one respondent's data\n",
        "def sim_one(id):\n",
        "    datlist = []\n",
        "    for t in range(1, n_tasks + 1):\n",
        "        dat = profiles.sample(n=n_alts).copy()\n",
        "        dat.insert(0, \"task\", t)\n",
        "        dat.insert(0, \"resp\", id)\n",
        "        dat[\"v\"] = (\n",
        "            dat[\"brand\"].map(b_util) +\n",
        "            dat[\"ad\"].map(a_util) +\n",
        "            dat[\"price\"].apply(p_util)\n",
        "        ).round(10)\n",
        "        dat[\"e\"] = -np.log(-np.log(np.random.uniform(size=n_alts)))\n",
        "        dat[\"u\"] = dat[\"v\"] + dat[\"e\"]\n",
        "        dat[\"choice\"] = (dat[\"u\"] == dat[\"u\"].max()).astype(int)\n",
        "        datlist.append(dat)\n",
        "    return pd.concat(datlist, ignore_index=True)\n",
        "\n",
        "# simulate data for all respondents\n",
        "conjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n",
        "\n",
        "# remove values unobservable to the researcher\n",
        "conjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n",
        "\n",
        "# clean up\n",
        "for name in dir():\n",
        "    if name != \"conjoint_data\":\n",
        "        del globals()[name]\n",
        "\n",
        "print(conjoint_data.head())"
      ],
      "id": "ef9f1724",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::::\n",
        "\n",
        "\n",
        "\n",
        "## 3. Preparing the Data for Estimation\n",
        "\n",
        "The \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n",
        "\n",
        "_todo: reshape and prep the data_ DONE"
      ],
      "id": "f7a808e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conjoint_data['brand_netflix'] = (conjoint_data['brand'] == 'N').astype(int)\n",
        "conjoint_data['brand_prime'] = (conjoint_data['brand'] == 'P').astype(int)\n",
        "conjoint_data['ads_yes'] = (conjoint_data['ad'] == 'Yes').astype(int)\n",
        "\n",
        "X = conjoint_data[['brand_netflix', 'brand_prime', 'ads_yes', 'price']].values\n",
        "y = conjoint_data['choice'].values\n",
        "\n",
        "respondent = conjoint_data['resp'].values\n",
        "task = conjoint_data['task'].values\n",
        "\n",
        "print(conjoint_data.head())"
      ],
      "id": "5871f6a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Estimation via Maximum Likelihood\n",
        "\n",
        "We implement the MNL model using maximum likelihood estimation. The log-likelihood function is optimized using the BFGS algorithm to find the parameter estimates and their standard errors.\n"
      ],
      "id": "2fd5d9d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def mnl(beta, X, y, ids):\n",
        "    v = X @ beta\n",
        "    \n",
        "    unique_tasks = np.unique(ids)\n",
        "    log_likelihood = 0.0\n",
        "    for t in unique_tasks:\n",
        "        idx = (ids == t)\n",
        "        v_t = v[idx]\n",
        "        y_t = y[idx]\n",
        "\n",
        "        denom = np.log(np.sum(np.exp(v_t)))\n",
        "        log_likelihood += np.sum(y_t * (v_t - denom))\n",
        "\n",
        "    return -log_likelihood\n",
        "\n",
        "task_ids = conjoint_data['resp'].astype(str) + \"_\" + conjoint_data['task'].astype(str)\n",
        "task_ids = task_ids.values"
      ],
      "id": "c2eeff7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we use scipy.optimize to find the MLEs for the Netflix, Prime, Ads and Price parameter.\n"
      ],
      "id": "74084f01"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# Minimize the log-likelihood\n",
        "result = minimize(\n",
        "    mnl,\n",
        "    x0=np.zeros(4),\n",
        "    args=(X, y, task_ids),\n",
        "    method='BFGS'\n",
        ")\n",
        "\n",
        "hessian = result.hess_inv\n",
        "error = np.sqrt(np.diag(hessian))\n",
        "ci_lower = result.x - 1.96 * error\n",
        "ci_upper = result.x + 1.96 * error\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'Parameter': ['Netflix', 'Prime', 'Ads', 'Price'],\n",
        "    'MLE Estimate': result.x,\n",
        "    'Std. Error': error,\n",
        "    '95% CI Lower': ci_lower,\n",
        "    '95% CI Upper': ci_upper\n",
        "})\n",
        "\n",
        "print(\"\\nMaximum Likelihood Estimation Results:\")\n",
        "print(results_df.to_string(index=False))"
      ],
      "id": "39a3f5d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Estimation via Bayesian Methods\n",
        "\n",
        "We implement a Metropolis-Hastings MCMC sampler to estimate the posterior distribution of the parameters. The sampler uses:\n",
        "- 11,000 total steps with 1,000 burn-in iterations\n",
        "- N(0,5) priors for binary variable coefficients\n",
        "- N(0,1) prior for the price coefficient\n",
        "- Multivariate normal proposal distribution\n"
      ],
      "id": "cff7944d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: bayesian-estimation\n",
        "#| code-fold: true\n",
        "import numpy as np\n",
        "\n",
        "def log_prior(beta):\n",
        "    # beta: array of length 4\n",
        "    # First 3: N(0, 5^2), Last: N(0, 1^2)\n",
        "    lp = 0\n",
        "    lp += -0.5 * ((beta[0] / 5) ** 2 + np.log(2 * np.pi * 25))\n",
        "    lp += -0.5 * ((beta[1] / 5) ** 2 + np.log(2 * np.pi * 25))\n",
        "    lp += -0.5 * ((beta[2] / 5) ** 2 + np.log(2 * np.pi * 25))\n",
        "    lp += -0.5 * ((beta[3] / 1) ** 2 + np.log(2 * np.pi * 1))\n",
        "    return lp\n",
        "\n",
        "def log_posterior(beta, X, y, task_ids):\n",
        "    ll = -mnl(beta, X, y, task_ids)\n",
        "    lp = log_prior(beta)\n",
        "    return ll + lp\n",
        "\n",
        "n_steps = 11000\n",
        "burn_in = 1000\n",
        "np.random.seed(42)\n",
        "\n",
        "beta_current = np.zeros(4)\n",
        "samples = np.zeros((n_steps, 4))\n",
        "log_post_current = log_posterior(beta_current, X, y, task_ids)\n",
        "\n",
        "proposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n",
        "\n",
        "for step in range(n_steps):\n",
        "    beta_proposal = beta_current + np.random.normal(0, proposal_sd)\n",
        "    log_post_proposal = log_posterior(beta_proposal, X, y, task_ids)\n",
        "    \n",
        "    log_ratio = log_post_proposal - log_post_current\n",
        "    \n",
        "    if np.log(np.random.uniform()) < log_ratio:\n",
        "        beta_current = beta_proposal\n",
        "        log_post_current = log_post_proposal\n",
        "    \n",
        "    samples[step] = beta_current\n",
        "\n",
        "posterior_samples = samples[burn_in:]\n",
        "\n",
        "posterior_means = np.mean(posterior_samples, axis=0)\n",
        "posterior_std = np.std(posterior_samples, axis=0)\n",
        "posterior_ci_lower = np.percentile(posterior_samples, 2.5, axis=0)\n",
        "posterior_ci_upper = np.percentile(posterior_samples, 97.5, axis=0)\n",
        "\n",
        "bayesian_results = pd.DataFrame({\n",
        "    'Parameter': ['Netflix', 'Prime', 'Ads', 'Price'],\n",
        "    'Posterior Mean': posterior_means,\n",
        "    'Posterior Std': posterior_std,\n",
        "    '95% CI Lower': posterior_ci_lower,\n",
        "    '95% CI Upper': posterior_ci_upper\n",
        "})\n",
        "\n",
        "print(\"\\nBayesian Estimation Results:\")\n",
        "print(bayesian_results.to_string(index=False))"
      ],
      "id": "bayesian-estimation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare the results from both estimation methods and discuss their implications for understanding consumer preferences in the streaming service market.\n"
      ],
      "id": "5fd3c93e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(posterior_samples[:,0])\n",
        "plt.title(\"Trace plot: beta_Netflix\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Value\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(posterior_samples[:,0], bins=30, density=True)\n",
        "plt.title(\"Posterior: beta_Netflix\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "d493b171",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results from both estimation methods provide valuable insights into consumer preferences for streaming services. The parameter estimates indicate:\n"
      ],
      "id": "6eeff76a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    'Parameter': ['Netflix', 'Prime', 'Ads', 'Price'],\n",
        "    'MLE_Estimate': result.x,\n",
        "    'MLE_StdErr': error,\n",
        "    'MLE_95CI_Lower': ci_lower,\n",
        "    'MLE_95CI_Upper': ci_upper,\n",
        "    'Bayes_Mean': means,\n",
        "    'Bayes_StdDev': stds,\n",
        "    'Bayes_95CI_Lower': ci_lower,\n",
        "    'Bayes_95CI_Upper': ci_upper\n",
        "})\n",
        "\n",
        "print(summary_df)"
      ],
      "id": "a0aeef15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Discussion\n",
        "\n",
        "_todo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does $\\beta_\\text{Netflix} > \\beta_\\text{Prime}$ mean? Does it make sense that $\\beta_\\text{price}$ is negative?_\n",
        "\n",
        "To simulate data from a multi-level (random-parameter or hierarchical) model, we would allow each respondent to have their own set of preference parameters, rather than assuming everyone shares the same coefficients. This means, for each respondent, we would draw their individual-level betas from a population distribution (such as a multivariate normal centered at the overall mean). In estimation, we would use hierarchical Bayesian methods or mixed logit models to estimate both the distribution of preferences across the population and the individual-level parameters. This approach captures real-world heterogeneity in preferences and is more realistic for analyzing actual conjoint data.\n",
        "\n",
        "_todo: At a high level, discuss what change you would need to make in order to simulate data from --- and estimate the parameters of --- a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze \"real world\" conjoint data._\n",
        "\n",
        "To move from a standard multinomial logit model to a multi-level (random-parameter or hierarchical) model, both the simulation and estimation processes need to be adapted to account for individual-level heterogeneity in preferences. In the simulation stage, rather than assigning the same set of part-worth utilities (betas) to every respondent, we would generate a unique set of coefficients for each individual. This is typically done by drawing each respondent’s betas from a population-level distribution, such as a multivariate normal distribution with a specified mean and covariance matrix. Each respondent’s choices would then be simulated using their own set of betas, allowing the simulated data to reflect realistic variation in preferences across individuals.\n",
        "For estimation, we would use a hierarchical modeling approach, such as hierarchical Bayes or a mixed logit model. In these models, each respondent’s coefficients are treated as random effects, assumed to be drawn from a common population distribution whose parameters are also estimated from the data. The estimation process thus involves recovering both the distribution of preferences in the population and the individual-level coefficients. This is typically achieved using advanced computational methods, such as Markov Chain Monte Carlo (MCMC) for Bayesian estimation or simulated maximum likelihood for mixed logit models. By making these changes, the model can capture the true diversity of preferences found in real-world conjoint data, providing richer and more accurate insights into consumer choice behavior.\n"
      ],
      "id": "455a51da"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\timon\\AppData\\Local\\Programs\\Python\\Python312\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}